# worker/certificate_worker.py - Firebase Storage ë²„í‚· ë¬¸ì œ í•´ê²°

import os
import io
import time
import logging
import signal
import sys
from datetime import datetime, timedelta, timezone
from typing import Optional, Dict, List, Tuple, Any
import pandas as pd
import firebase_admin
from firebase_admin import credentials, firestore, storage
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from pathlib import Path
import json

# ===================================================================
# ë¡œê¹… ì„¤ì •
# ===================================================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger('CertificateWorker')

# ===================================================================
# í™˜ê²½ë³€ìˆ˜ ê²€ì¦
# ===================================================================
required_env_vars = ['type', 'project_id', 'private_key', 'client_email']
for var in required_env_vars:
    if not os.environ.get(var):
        logger.error(f"í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ ëˆ„ë½: {var}")
        sys.exit(1)

# ì„¤ì •ê°’
POLL_INTERVAL_SECONDS = max(30, int(os.getenv('POLL_INTERVAL_SECONDS', '45')))
BATCH_SIZE = min(20, int(os.getenv('BATCH_SIZE', '15')))
MASTER_FILENAME = "master_certificates.xlsx"
MAX_RETRY_COUNT = int(os.getenv('MAX_RETRY_COUNT', '5'))
HEALTH_CHECK_INTERVAL = 300

# ğŸ”§ Firebase Storage ë²„í‚· ì´ë¦„ ê²°ì •
FIREBASE_STORAGE_BUCKET = os.getenv('FIREBASE_STORAGE_BUCKET')
if not FIREBASE_STORAGE_BUCKET:
    project_id = os.environ['project_id']
    # ìƒˆë¡œìš´ FirebaseëŠ” .firebasestorage.app í˜•ì‹ ì‚¬ìš©
    FIREBASE_STORAGE_BUCKET = f"{project_id}.firebasestorage.app"

logger.info(f"ğŸš€ ì„¤ì • ì™„ë£Œ - í´ë§:{POLL_INTERVAL_SECONDS}ì´ˆ, ë°°ì¹˜:{BATCH_SIZE}ê°œ")
logger.info(f"ğŸª£ Storage ë²„í‚·: {FIREBASE_STORAGE_BUCKET}")

# ===================================================================
# Firebase ì´ˆê¸°í™”
# ===================================================================
def initialize_firebase():
    """Firebase ì´ˆê¸°í™”"""
    try:
        if firebase_admin._apps:
            return
        
        firebase_creds = {
            "type": os.environ["type"],
            "project_id": os.environ["project_id"],
            "private_key_id": os.environ.get("private_key_id", ""),
            "private_key": os.environ["private_key"].replace('\\n', '\n'),
            "client_email": os.environ["client_email"],
            "client_id": os.environ.get("client_id", ""),
            "auth_uri": "https://accounts.google.com/o/oauth2/auth",
            "token_uri": "https://oauth2.googleapis.com/token",
            "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
            "client_x509_cert_url": os.environ.get("client_x509_cert_url", "")
        }
        
        cred = credentials.Certificate(firebase_creds)
        
        # ğŸ”§ ì˜¬ë°”ë¥¸ Storage ë²„í‚· ì´ë¦„ ì‚¬ìš©
        app = firebase_admin.initialize_app(cred, {
            'storageBucket': FIREBASE_STORAGE_BUCKET
        })
        
        logger.info("âœ… Firebase ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"âœ… Firebase Storage ë²„í‚·: {FIREBASE_STORAGE_BUCKET}")
        
    except Exception as e:
        logger.error(f"âŒ Firebase ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        sys.exit(1)

# Firebase ì´ˆê¸°í™”
initialize_firebase()
db = firestore.client()
bucket = storage.bucket()

# ===================================================================
# ì•ˆì „í•œ ì¢…ë£Œ ê´€ë¦¬
# ===================================================================
shutdown_flag = False
current_operations = set()
operations_lock = threading.Lock()

def signal_handler(signum, frame):
    global shutdown_flag
    logger.info(f"ğŸ›‘ ì¢…ë£Œ ì‹œê·¸ë„ ìˆ˜ì‹  - ì•ˆì „í•œ ì¢…ë£Œ ì‹œì‘")
    shutdown_flag = True
    
    with operations_lock:
        if current_operations:
            logger.info(f"ğŸ“‹ {len(current_operations)}ê°œ ì‘ì—… ì™„ë£Œ ëŒ€ê¸°")

signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

def log_operation_start(operation_id):
    with operations_lock:
        current_operations.add(operation_id)

def log_operation_end(operation_id):
    with operations_lock:
        current_operations.discard(operation_id)

# ===================================================================
# ìˆ˜ë£Œì¦ ì¡°íšŒ í•¨ìˆ˜ë“¤
# ===================================================================
def get_pending_certificates_debug(limit=50):
    """ë””ë²„ê¹…ì´ í¬í•¨ëœ ìˆ˜ë£Œì¦ ì¡°íšŒ"""
    operation_id = f"get_pending_debug_{int(time.time())}"
    log_operation_start(operation_id)
    
    try:
        logger.info("ğŸ” ìƒì„¸ ë””ë²„ê¹… ìˆ˜ë£Œì¦ ì¡°íšŒ ì‹œì‘")
        
        # 1ë‹¨ê³„: ì „ì²´ ìˆ˜ë£Œì¦ ë¬¸ì„œ ìˆ˜ í™•ì¸
        try:
            all_certs_query = db.collection_group('completedCertificates').limit(100)
            all_docs = list(all_certs_query.stream())
            logger.info(f"ğŸ“Š ì „ì²´ ìˆ˜ë£Œì¦ ë¬¸ì„œ: {len(all_docs)}ê°œ ë°œê²¬")
            
            # ìƒ˜í”Œ ë¶„ì„
            analysis = {
                'has_pdf_url': 0,
                'sent_to_admin_true': 0,
                'excel_updated_false': 0,
                'retry_over_limit': 0,
                'processable': 0
            }
            
            for doc in all_docs[:20]:
                try:
                    data = doc.to_dict()
                    
                    if data.get('pdfUrl', '').strip():
                        analysis['has_pdf_url'] += 1
                    
                    if data.get('sentToAdmin', False):
                        analysis['sent_to_admin_true'] += 1
                    
                    if not data.get('excelUpdated', True):
                        analysis['excel_updated_false'] += 1
                    
                    retry_count = data.get('retryCount', 0)
                    if retry_count >= MAX_RETRY_COUNT:
                        analysis['retry_over_limit'] += 1
                    
                    # ì²˜ë¦¬ ê°€ëŠ¥í•œ ë¬¸ì„œ ì¡°ê±´
                    if (data.get('pdfUrl', '').strip() and 
                        data.get('sentToAdmin', False) and 
                        not data.get('excelUpdated', True) and 
                        retry_count < MAX_RETRY_COUNT):
                        analysis['processable'] += 1
                        
                except Exception:
                    continue
            
            logger.info(f"ğŸ“ˆ ìƒ˜í”Œ ë¶„ì„ ê²°ê³¼ (ìƒìœ„ 20ê°œ):")
            logger.info(f"  - PDF URL ìˆìŒ: {analysis['has_pdf_url']}/20")
            logger.info(f"  - sentToAdmin=true: {analysis['sent_to_admin_true']}/20")
            logger.info(f"  - excelUpdated=false: {analysis['excel_updated_false']}/20")
            logger.info(f"  - ì¬ì‹œë„ í•œë„ ì´ˆê³¼: {analysis['retry_over_limit']}/20")
            logger.info(f"  - ì²˜ë¦¬ ê°€ëŠ¥í•œ ë¬¸ì„œ: {analysis['processable']}/20")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ì „ì²´ ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        # 2ë‹¨ê³„: ì‹¤ì œ ì¿¼ë¦¬ ì‹¤í–‰
        logger.info("ğŸ” ì‹¤ì œ í•„í„°ë§ ì¿¼ë¦¬ ì‹¤í–‰")
        
        queries = [
            ("ê¸°ë³¸", db.collection_group('completedCertificates').limit(limit)),
            ("excelë¯¸ì™„ë£Œ", db.collection_group('completedCertificates').where('excelUpdated', '==', False).limit(limit)),
            ("ì „ì†¡ì™„ë£Œ", db.collection_group('completedCertificates').where('sentToAdmin', '==', True).limit(limit)),
        ]
        
        for name, query in queries:
            try:
                docs = list(query.stream())
                logger.info(f"  ğŸ“‹ {name}: {len(docs)}ê°œ ë¬¸ì„œ")
            except Exception as e:
                logger.warning(f"  âŒ {name} ì‹¤íŒ¨: {e}")
        
        # 3ë‹¨ê³„: ìµœì¢… ì²˜ë¦¬ ëŒ€ìƒ ìˆ˜ë£Œì¦ ì¡°íšŒ
        logger.info("ğŸ” ìµœì¢… ì²˜ë¦¬ ëŒ€ìƒ ì¡°íšŒ")
        
        try:
            final_query = db.collection_group('completedCertificates') \
                           .where('sentToAdmin', '==', True) \
                           .where('excelUpdated', '==', False) \
                           .limit(limit)
            
            results = []
            skip_reasons = {
                'no_pdf_url': 0,
                'retry_exceeded': 0,
                'path_error': 0,
                'data_error': 0
            }
            
            processed_count = 0
            for doc in final_query.stream():
                if shutdown_flag or processed_count >= limit:
                    break
                    
                processed_count += 1
                
                try:
                    data = doc.to_dict()
                    
                    # PDF URL ì²´í¬
                    pdf_url = data.get('pdfUrl', '').strip()
                    if not pdf_url:
                        skip_reasons['no_pdf_url'] += 1
                        continue
                    
                    # ì¬ì‹œë„ íšŸìˆ˜ ì²´í¬
                    retry_count = data.get('retryCount', 0)
                    if retry_count >= MAX_RETRY_COUNT:
                        skip_reasons['retry_exceeded'] += 1
                        # ì¬ì‹œë„ ì¹´ìš´í„° ë¦¬ì…‹ ì˜µì…˜
                        reset_enabled = os.getenv('RESET_HIGH_RETRY_COUNT', 'false').lower() == 'true'
                        if reset_enabled and retry_count <= 10:
                            logger.info(f"ğŸ”„ ì¬ì‹œë„ ì¹´ìš´í„° ë¦¬ì…‹: {doc.id[:12]}... (í˜„ì¬: {retry_count})")
                            try:
                                doc.reference.update({
                                    'retryCount': 0,
                                    'resetAt': firestore.SERVER_TIMESTAMP,
                                    'resetReason': 'worker_reset'
                                })
                                # ë¦¬ì…‹ í›„ ê³„ì† ì²˜ë¦¬
                            except Exception as reset_err:
                                logger.debug(f"ë¦¬ì…‹ ì‹¤íŒ¨: {reset_err}")
                                continue
                        else:
                            continue
                    
                    # ê²½ë¡œì—ì„œ UID ì¶”ì¶œ
                    path_parts = doc.reference.path.split('/')
                    if len(path_parts) < 4:
                        skip_reasons['path_error'] += 1
                        continue
                    
                    user_uid = path_parts[1]
                    cert_id = doc.id
                    
                    if not user_uid or not cert_id:
                        skip_reasons['data_error'] += 1
                        continue
                    
                    results.append((user_uid, cert_id, data))
                    
                    if len(results) <= 3:
                        lecture_title = data.get('lectureTitle', 'ì œëª©ì—†ìŒ')
                        logger.info(f"âœ… ì²˜ë¦¬ ëŒ€ìƒ: {user_uid[:8]}.../{cert_id[:8]}... - {lecture_title[:30]}...")
                        
                except Exception as e:
                    skip_reasons['data_error'] += 1
                    logger.debug(f"ë¬¸ì„œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue
            
            # ê²°ê³¼ ë¡œê¹…
            logger.info(f"ğŸ¯ ìµœì¢… ê²°ê³¼:")
            logger.info(f"  ğŸ“Š ê²€ìƒ‰ëœ ë¬¸ì„œ: {processed_count}ê°œ")
            logger.info(f"  âœ… ì²˜ë¦¬ ê°€ëŠ¥: {len(results)}ê°œ")
            logger.info(f"  âŒ ìŠ¤í‚µëœ ì´ìœ :")
            for reason, count in skip_reasons.items():
                if count > 0:
                    reason_kr = {
                        'no_pdf_url': 'PDF URL ì—†ìŒ',
                        'retry_exceeded': 'ì¬ì‹œë„ í•œë„ ì´ˆê³¼',
                        'path_error': 'ê²½ë¡œ ì˜¤ë¥˜',
                        'data_error': 'ë°ì´í„° ì˜¤ë¥˜'
                    }.get(reason, reason)
                    logger.info(f"    - {reason_kr}: {count}ê°œ")
            
            if len(results) == 0 and processed_count > 0:
                logger.warning("âš ï¸ ë¬¸ì„œëŠ” ìˆì§€ë§Œ ì²˜ë¦¬ ê°€ëŠ¥í•œ ìˆ˜ë£Œì¦ì´ ì—†ìŠµë‹ˆë‹¤!")
                logger.warning("ğŸ’¡ í•´ê²° ë°©ë²•:")
                logger.warning("  1. ì•±ì—ì„œ 'ê´€ë¦¬ì ì „ì†¡' ë²„íŠ¼ í´ë¦­ìœ¼ë¡œ pdfUrl ìƒì„±")
                logger.warning("  2. í™˜ê²½ë³€ìˆ˜ RESET_HIGH_RETRY_COUNT=true ì„¤ì •ìœ¼ë¡œ ì¬ì‹œë„ ë¦¬ì…‹")
                logger.warning("  3. Firebaseì—ì„œ sentToAdmin=true í™•ì¸")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ ìµœì¢… ì¿¼ë¦¬ ì‹¤íŒ¨: {e}")
            return []
            
    except Exception as e:
        logger.error(f"âŒ ìˆ˜ë£Œì¦ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return []
    finally:
        log_operation_end(operation_id)

def get_user_info(user_uid):
    """ì‚¬ìš©ì ì •ë³´ ì¡°íšŒ (ìºì‹±)"""
    if not hasattr(get_user_info, 'cache'):
        get_user_info.cache = {}
    
    if user_uid in get_user_info.cache:
        return get_user_info.cache[user_uid]
    
    try:
        user_doc = db.collection('users').document(user_uid).get()
        
        if user_doc.exists:
            user_data = user_doc.to_dict()
            user_info = {
                'name': user_data.get('name', ''),
                'phone': user_data.get('phone', ''),
                'email': user_data.get('email', '')
            }
            
            if len(get_user_info.cache) < 1000:
                get_user_info.cache[user_uid] = user_info
            
            return user_info
        else:
            empty_info = {'name': '', 'phone': '', 'email': ''}
            get_user_info.cache[user_uid] = empty_info
            return empty_info
            
    except Exception as e:
        logger.debug(f"ì‚¬ìš©ì ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {'name': '', 'phone': '', 'email': ''}

# ===================================================================
# Excel ì²˜ë¦¬ í•¨ìˆ˜ë“¤
# ===================================================================
def load_master_excel():
    """ë§ˆìŠ¤í„° ì—‘ì…€ ë¡œë“œ"""
    operation_id = f"load_excel_{int(time.time())}"
    log_operation_start(operation_id)
    
    try:
        # ğŸ”§ ì˜¬ë°”ë¥¸ ë²„í‚·ìœ¼ë¡œ Firebase Storageì—ì„œ ë¡œë“œ
        try:
            logger.info(f"ğŸ“¥ Firebase Storageì—ì„œ ì—‘ì…€ ë¡œë“œ ì‹œë„: {FIREBASE_STORAGE_BUCKET}/{MASTER_FILENAME}")
            master_blob = bucket.blob(MASTER_FILENAME)
            
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not master_blob.exists():
                logger.info("âš ï¸ ë§ˆìŠ¤í„° ì—‘ì…€ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ, ìƒˆë¡œ ìƒì„±")
                return create_empty_dataframe()
            
            existing_bytes = master_blob.download_as_bytes()
            logger.info(f"ğŸ“¥ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {len(existing_bytes)} bytes")
            
            excel_buffer = io.BytesIO(existing_bytes)
            df = pd.read_excel(excel_buffer, engine='openpyxl')
            
            expected_columns = [
                'ì—…ë°ì´íŠ¸ ë‚ ì§œ', 'ì‚¬ìš©ì UID', 'ì „í™”ë²ˆí˜¸', 'ì´ë©”ì¼', 
                'ì‚¬ìš©ì ì´ë¦„', 'ê°•ì˜ ì œëª©', 'ë°œê¸‰ ì¼ì‹œ', 'PDF URL'
            ]
            
            if not all(col in df.columns for col in expected_columns):
                logger.warning("âš ï¸ ì—‘ì…€ ì»¬ëŸ¼ êµ¬ì¡° ì´ìƒ, ìƒˆë¡œ ìƒì„±")
                return create_empty_dataframe()
            
            if len(df) > 15000:
                logger.warning("âš ï¸ ë°ì´í„° í¬ê¸° ì œí•œìœ¼ë¡œ ìµœê·¼ 10,000í–‰ë§Œ ìœ ì§€")
                df = df.tail(10000).reset_index(drop=True)
            
            logger.info(f"âœ… ì—‘ì…€ ë¡œë“œ ì™„ë£Œ (í–‰ ìˆ˜: {len(df)})")
            return df
            
        except Exception as firebase_error:
            logger.warning(f"âš ï¸ Firebase Storage ë¡œë“œ ì‹¤íŒ¨: {firebase_error}")
            logger.info("âš ï¸ ìƒˆ íŒŒì¼ ìƒì„±")
            return create_empty_dataframe()
            
    except Exception as e:
        logger.error(f"âŒ ì—‘ì…€ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return create_empty_dataframe()
    finally:
        log_operation_end(operation_id)

def create_empty_dataframe():
    """ë¹ˆ DataFrame ìƒì„±"""
    return pd.DataFrame(columns=[
        'ì—…ë°ì´íŠ¸ ë‚ ì§œ',
        'ì‚¬ìš©ì UID',
        'ì „í™”ë²ˆí˜¸',
        'ì´ë©”ì¼',
        'ì‚¬ìš©ì ì´ë¦„',
        'ê°•ì˜ ì œëª©',
        'ë°œê¸‰ ì¼ì‹œ',
        'PDF URL'
    ])

def save_master_excel(df):
    """ë§ˆìŠ¤í„° ì—‘ì…€ ì €ì¥"""
    operation_id = f"save_excel_{int(time.time())}"
    log_operation_start(operation_id)
    
    try:
        if len(df) > 20000:
            logger.warning("âš ï¸ ì €ì¥ í¬ê¸° ì œí•œìœ¼ë¡œ ìµœê·¼ 15,000í–‰ë§Œ ì €ì¥")
            df = df.tail(15000).reset_index(drop=True)
        
        out_buffer = io.BytesIO()
        with pd.ExcelWriter(out_buffer, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name='Certificates')
        out_buffer.seek(0)
        
        # ë¡œì»¬ ë°±ì—…
        backup_path = Path(f'/tmp/backup_{int(time.time())}.xlsx')
        try:
            with open(backup_path, 'wb') as f:
                f.write(out_buffer.getvalue())
            logger.info("ğŸ’¾ ë¡œì»¬ ë°±ì—… ì™„ë£Œ")
        except Exception:
            logger.debug("ë¡œì»¬ ë°±ì—… ì‹¤íŒ¨")
        
        # ğŸ”§ ì˜¬ë°”ë¥¸ ë²„í‚·ìœ¼ë¡œ Firebase Storage ì—…ë¡œë“œ
        max_retries = 3
        for attempt in range(max_retries):
            try:
                out_buffer.seek(0)
                logger.info(f"ğŸ“¤ Firebase Storage ì—…ë¡œë“œ ì‹œë„ {attempt + 1}/{max_retries}: {FIREBASE_STORAGE_BUCKET}/{MASTER_FILENAME}")
                
                master_blob = bucket.blob(MASTER_FILENAME)
                
                master_blob.upload_from_file(
                    out_buffer,
                    content_type='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
                )
                
                logger.info(f"âœ… ì—‘ì…€ ì €ì¥ ì™„ë£Œ (ì´ {len(df)}í–‰)")
                logger.info(f"âœ… Firebase Storage ê²½ë¡œ: {FIREBASE_STORAGE_BUCKET}/{MASTER_FILENAME}")
                
                # ë°±ì—… íŒŒì¼ ì •ë¦¬
                try:
                    backup_path.unlink()
                except Exception:
                    pass
                
                return True
                
            except Exception as e:
                logger.warning(f"âš ï¸ ì—‘ì…€ ì €ì¥ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                if 'bucket does not exist' in str(e).lower():
                    logger.error(f"âŒ Storage ë²„í‚·ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {FIREBASE_STORAGE_BUCKET}")
                    logger.error("ğŸ’¡ í•´ê²° ë°©ë²•:")
                    logger.error("   1. Firebase Console â†’ Storage â†’ Get started")
                    logger.error(f"   2. í™˜ê²½ë³€ìˆ˜ í™•ì¸: FIREBASE_STORAGE_BUCKET={FIREBASE_STORAGE_BUCKET}")
                
                if attempt < max_retries - 1:
                    time.sleep(2)
                else:
                    return False
        
        return False
        
    except Exception as e:
        logger.error(f"âŒ ì—‘ì…€ ì €ì¥ ì¤‘ ì˜ˆì™¸: {e}")
        return False
    finally:
        log_operation_end(operation_id)

# ===================================================================
# ìˆ˜ë£Œì¦ ì²˜ë¦¬ í•¨ìˆ˜ë“¤
# ===================================================================
def process_certificate(user_uid, cert_id, cert_data, df):
    """ë‹¨ì¼ ìˆ˜ë£Œì¦ ì²˜ë¦¬"""
    try:
        user_info = get_user_info(user_uid)
        
        lecture_title = cert_data.get('lectureTitle', cert_id)
        pdf_url = cert_data.get('pdfUrl', '')
        
        if not pdf_url.strip():
            raise ValueError("PDF URLì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
        
        issued_at = cert_data.get('issuedAt')
        if hasattr(issued_at, 'to_datetime'):
            issued_str = issued_at.to_datetime().strftime('%Y-%m-%d %H:%M:%S')
        else:
            issued_str = datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')
        
        # ì¤‘ë³µ í™•ì¸
        existing_mask = (
            (df['ì‚¬ìš©ì UID'] == user_uid) & 
            (df['ê°•ì˜ ì œëª©'] == lecture_title) & 
            (df['PDF URL'] == pdf_url)
        )
        
        if existing_mask.any():
            logger.info(f"âš ï¸ ì¤‘ë³µ ìˆ˜ë£Œì¦ ìŠ¤í‚µ: {user_uid[:8]}.../{lecture_title[:20]}...")
            return True, df
        
        updated_date = datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')
        new_row = pd.DataFrame([{
            'ì—…ë°ì´íŠ¸ ë‚ ì§œ': updated_date,
            'ì‚¬ìš©ì UID': user_uid,
            'ì „í™”ë²ˆí˜¸': user_info['phone'],
            'ì´ë©”ì¼': user_info['email'],
            'ì‚¬ìš©ì ì´ë¦„': user_info['name'],
            'ê°•ì˜ ì œëª©': lecture_title,
            'ë°œê¸‰ ì¼ì‹œ': issued_str,
            'PDF URL': pdf_url
        }])
        
        df = pd.concat([df, new_row], ignore_index=True)
        
        logger.info(f"âœ… ìˆ˜ë£Œì¦ ì²˜ë¦¬ ì™„ë£Œ: {user_uid[:8]}.../{cert_id[:8]}... - {lecture_title[:30]}...")
        return True, df
        
    except Exception as e:
        logger.warning(f"âš ï¸ ìˆ˜ë£Œì¦ ì²˜ë¦¬ ì‹¤íŒ¨ ({user_uid[:8]}.../{cert_id[:8]}...): {e}")
        
        try:
            cert_ref = db.collection('users').document(user_uid) \
                         .collection('completedCertificates').document(cert_id)
            cert_ref.update({
                'processingError': str(e)[:200],
                'errorOccurredAt': firestore.SERVER_TIMESTAMP,
                'retryCount': firestore.Increment(1)
            })
        except Exception:
            pass
        
        return False, df

def update_certificate_flags_batch(processed_certs, success=True):
    """ë°°ì¹˜ í”Œë˜ê·¸ ì—…ë°ì´íŠ¸"""
    operation_id = f"update_flags_{int(time.time())}"
    log_operation_start(operation_id)
    
    try:
        def update_single_flag(cert_info):
            user_uid, cert_id, cert_data = cert_info
            try:
                cert_ref = db.collection('users').document(user_uid) \
                             .collection('completedCertificates').document(cert_id)
                
                if success:
                    update_data = {
                        'excelUpdated': True,
                        'processedAt': firestore.SERVER_TIMESTAMP,
                        'processedBy': 'fixed_storage_worker_v1',
                        'workerProcessed': True
                    }
                    
                    if 'processingError' in cert_data:
                        update_data['processingError'] = firestore.DELETE_FIELD
                    if 'readyForExcel' in cert_data:
                        update_data['readyForExcel'] = False
                    
                    cert_ref.update(update_data)
                    return "âœ…"
                    
                else:
                    cert_ref.update({
                        'excelSaveError': True,
                        'excelSaveErrorAt': firestore.SERVER_TIMESTAMP,
                        'retryCount': firestore.Increment(1)
                    })
                    return "âš ï¸"
                    
            except Exception as e:
                logger.debug(f"í”Œë˜ê·¸ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                return "âŒ"
        
        with ThreadPoolExecutor(max_workers=5) as executor:
            results = list(executor.map(update_single_flag, processed_certs))
        
        success_count = sum(1 for r in results if r == 'âœ…')
        logger.info(f"âœ… í”Œë˜ê·¸ ì—…ë°ì´íŠ¸: {success_count}/{len(results)}")
        
    except Exception as e:
        logger.error(f"âŒ ë°°ì¹˜ í”Œë˜ê·¸ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    finally:
        log_operation_end(operation_id)

# ===================================================================
# ë°°ì¹˜ ì²˜ë¦¬
# ===================================================================
def process_batch():
    """ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰"""
    operation_id = f"batch_{int(time.time())}"
    log_operation_start(operation_id)
    
    try:
        batch_start_time = datetime.now(timezone.utc)
        
        pending_certs = get_pending_certificates_debug(limit=BATCH_SIZE)
        
        if not pending_certs:
            logger.info("ğŸ˜´ ì²˜ë¦¬í•  ìˆ˜ë£Œì¦ì´ ì—†ìŠµë‹ˆë‹¤ - ìƒì„¸ ë¶„ì„ ì™„ë£Œ")
            return
        
        logger.info(f"ğŸš€ {len(pending_certs)}ê°œ ìˆ˜ë£Œì¦ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘")
        
        df = load_master_excel()
        original_row_count = len(df)
        
        success_count = 0
        error_count = 0
        processed_certs = []
        
        for i, (user_uid, cert_id, cert_data) in enumerate(pending_certs, 1):
            if shutdown_flag:
                logger.info("ğŸ›‘ ì¢…ë£Œ í”Œë˜ê·¸ ê°ì§€, ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ë‹¨")
                break
            
            if len(pending_certs) > 5 and i % max(1, len(pending_certs) // 5) == 0:
                progress = (i / len(pending_certs)) * 100
                logger.info(f"ğŸ“Š ì²˜ë¦¬ ì§„í–‰ë¥ : {progress:.0f}% ({i}/{len(pending_certs)})")
            
            success, df = process_certificate(user_uid, cert_id, cert_data, df)
            
            if success:
                success_count += 1
                processed_certs.append((user_uid, cert_id, cert_data))
            else:
                error_count += 1
        
        if success_count > 0:
            new_row_count = len(df)
            logger.info(f"ğŸ“Š Excel ì €ì¥: {original_row_count}í–‰ â†’ {new_row_count}í–‰ (+{new_row_count - original_row_count})")
            
            excel_save_success = save_master_excel(df)
            update_certificate_flags_batch(processed_certs, success=excel_save_success)
            
            processing_time = (datetime.now(timezone.utc) - batch_start_time).total_seconds()
            
            if excel_save_success:
                logger.info(f"ğŸ‰ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ - âœ…ì„±ê³µ: {success_count}, âŒì‹¤íŒ¨: {error_count}, â±ï¸ì‹œê°„: {processing_time:.1f}ì´ˆ")
            else:
                logger.error(f"âŒ Excel ì €ì¥ ì‹¤íŒ¨ - ì¬ì‹œë„ ì˜ˆì •")
        else:
            logger.info(f"ğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ - ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ í•­ëª© ì—†ìŒ (âŒì‹¤íŒ¨: {error_count})")
        
    except Exception as e:
        logger.error(f"âŒ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    finally:
        log_operation_end(operation_id)

def update_health_status():
    """í—¬ìŠ¤ì²´í¬ íŒŒì¼ ì—…ë°ì´íŠ¸"""
    try:
        health_file = Path('/tmp/worker_healthy')
        health_data = {
            'status': 'healthy',
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'active_operations': len(current_operations),
            'poll_interval': POLL_INTERVAL_SECONDS,
            'batch_size': BATCH_SIZE,
            'storage_bucket': FIREBASE_STORAGE_BUCKET
        }
        
        with open(health_file, 'w') as f:
            json.dump(health_data, f)
            
    except Exception as e:
        logger.debug(f"í—¬ìŠ¤ íŒŒì¼ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

# ===================================================================
# ë©”ì¸ ì›Œì»¤ ë£¨í”„
# ===================================================================
def run_worker():
    """ë©”ì¸ ì›Œì»¤ ë£¨í”„"""
    logger.info(f"ğŸš€ Certificate Worker v3.3 ì‹œì‘ (Storage ë²„í‚· ë¬¸ì œ í•´ê²°)")
    logger.info(f"â±ï¸ í´ë§ ê°„ê²©: {POLL_INTERVAL_SECONDS}ì´ˆ")
    logger.info(f"ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}")
    logger.info(f"ğŸ”„ ìµœëŒ€ ì¬ì‹œë„: {MAX_RETRY_COUNT}")
    logger.info(f"ğŸ”§ ì¬ì‹œë„ ë¦¬ì…‹: {os.getenv('RESET_HIGH_RETRY_COUNT', 'false')}")
    logger.info(f"ğŸª£ Storage ë²„í‚·: {FIREBASE_STORAGE_BUCKET}")
    
    update_health_status()
    
    iteration = 0
    last_activity_time = None
    consecutive_empty_batches = 0
    
    while not shutdown_flag:
        try:
            iteration += 1
            
            if iteration % 10 == 0:
                update_health_status()
            
            batch_start_time = datetime.now(timezone.utc)
            
            if len(current_operations) > 10:
                logger.warning(f"âš ï¸ ë„ˆë¬´ ë§ì€ ë™ì‹œ ì‘ì—…: {len(current_operations)}ê°œ")
                time.sleep(5)
                continue
            
            # ë°°ì¹˜ ì²˜ë¦¬
            process_batch()
            
            consecutive_empty_batches += 1
            
            # ìƒíƒœ ë¡œê¹…
            if iteration % 10 == 0:
                logger.info(f"ğŸ“ˆ ìƒíƒœ - ë°˜ë³µ: {iteration}, í™œì„±ì‘ì—…: {len(current_operations)}ê°œ, ë²„í‚·: {FIREBASE_STORAGE_BUCKET}")
            
            # ë™ì  ëŒ€ê¸° ì‹œê°„
            if consecutive_empty_batches > 10:
                sleep_time = min(POLL_INTERVAL_SECONDS * 2, 120)
            else:
                sleep_time = POLL_INTERVAL_SECONDS
            
            # ì¸í„°ëŸ½íŠ¸ ê°€ëŠ¥í•œ ëŒ€ê¸°
            for _ in range(sleep_time):
                if shutdown_flag:
                    break
                time.sleep(1)
                
        except KeyboardInterrupt:
            logger.info("âŒ¨ï¸ í‚¤ë³´ë“œ ì¸í„°ëŸ½íŠ¸ - ì¢…ë£Œ")
            break
        except Exception as e:
            logger.error(f"âŒ ì›Œì»¤ ë£¨í”„ ì˜¤ë¥˜: {e}")
            time.sleep(min(POLL_INTERVAL_SECONDS, 60))
    
    # ì¢…ë£Œ ì •ë¦¬
    with operations_lock:
        if current_operations:
            logger.info(f"ğŸ”„ {len(current_operations)}ê°œ ì‘ì—… ì™„ë£Œ ëŒ€ê¸°...")
            for _ in range(30):
                if not current_operations:
                    break
                time.sleep(1)
    
    logger.info("ğŸ‘‹ Certificate Worker ì•ˆì „ ì¢…ë£Œ ì™„ë£Œ")

# ===================================================================
# ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸
# ===================================================================
if __name__ == "__main__":
    try:
        logger.info("ğŸ” ìµœì¢… ê²€ì¦ ì¤‘...")
        logger.info(f"ğŸª£ ì‚¬ìš©í•  Storage ë²„í‚·: {FIREBASE_STORAGE_BUCKET}")
        
        # Firebase ì—°ê²° í…ŒìŠ¤íŠ¸
        test_collection = db.collection('_worker_health')
        test_doc = test_collection.document(f"storage_test_{int(time.time())}")
        test_doc.set({
            'timestamp': firestore.SERVER_TIMESTAMP,
            'worker': 'fixed_storage_certificate_worker_v1',
            'startup_time': datetime.now(timezone.utc).isoformat(),
            'storage_bucket': FIREBASE_STORAGE_BUCKET
        })
        test_doc.delete()
        
        logger.info("âœ… ìµœì¢… ê²€ì¦ ì™„ë£Œ")
        
        # ì›Œì»¤ ì‹œì‘
        run_worker()
        
    except Exception as e:
        logger.error(f"âŒ ì›Œì»¤ ì‹œì‘ ì‹¤íŒ¨: {e}")
        sys.exit(1)